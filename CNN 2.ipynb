{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style('dark')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "We set a few flags for whether we are running locally or on Kaggle, or whether we want the reduced or full dataset. For the reduced dataset, the test set consists of samples from the full train set. For use during model evaluation, this has been augmented with reverse-engineered targets. This is simply to allow us to evaluate the AUC score on this set as well - it is never used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "IS_LOCAL = True  # Sets whether we are running locally or on kaggle\n",
    "USE_REDUCED = True  # Sets whether we should use the smaller dataset\n",
    "data_index = 2*int(IS_LOCAL) + int(USE_REDUCED)\n",
    "train_path = ('../input/santander-customer-transaction-prediction/train.csv',\n",
    "             '../input/santandersmall/train_small.csv',\n",
    "             'train.csv',\n",
    "             'train_small.csv')[data_index]\n",
    "test_path = ('../input/santander-customer-transaction-prediction/test.csv',\n",
    "             '../input/santandersmall/test_small_with_targets.csv',\n",
    "             'test.csv',\n",
    "             'test_small.csv')[data_index]\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in train_df.columns if col not in ['target', 'ID_code']]\n",
    "if not 'target' in test_df:\n",
    "    test_df['target'] = -1\n",
    "\n",
    "all_df = pd.concat([train_df, test_df], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing fake test samples\n",
    "It was discovered during the competition that some of the test samples were synthetic. Moreover, it was stated that not all of the data in the test set was used for evaluation, and the synthetic data corresponds to this. In order to achieve the best scores possible, the fake samples should be removed before calculating features such as counts. We calculate the indices of the fake rows below using the method provided by the Kaggle user YaG320. When using the reduced dataset, we have no synthetic samples. This is handled correctly by the algorithm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a5dfff85144b0f957192eeb90f0d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real: 70065\n",
      "Synthetic: 0\n"
     ]
    }
   ],
   "source": [
    "unique_count = np.zeros((test_df.shape[0], len(features)))\n",
    "\n",
    "for f, feature in tqdm(enumerate(features), total=len(features)):\n",
    "    _, i, c = np.unique(test_df[feature], return_counts=True, return_index=True)\n",
    "    unique_count[i[c == 1], f] += 1\n",
    "\n",
    "real_sample_indices = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n",
    "synthetic_sample_indices = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n",
    "print('Real:', len(real_sample_indices))\n",
    "print('Synthetic:', len(synthetic_sample_indices))\n",
    "\n",
    "del unique_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate counts\n",
    "Counts of the values of the different features is often a powerful feature in itself. We calculate these below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820e25e729444df4a74ee037a1e71ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_real_df = pd.concat([train_df, test_df.iloc[real_sample_indices, :]], sort=False)\n",
    "\n",
    "for feature in tqdm(features):\n",
    "    real_series = all_real_df[feature]\n",
    "    \n",
    "    # We only use the real samples to produce the count\n",
    "    counts = real_series.groupby(real_series).count()\n",
    "    \n",
    "    full_series = all_df[feature]\n",
    "    all_df[f'{feature}_count'] = full_series.map(counts)\n",
    "\n",
    "del all_real_df\n",
    "del real_series\n",
    "del full_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Properties\n",
    "Some statistical properties of the rows of data can also help the classifier make correct predictions by revealing connections that may otherwise be difficult to discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['sum'] = all_df[features].sum(axis=1)\n",
    "all_df['mean'] = all_df[features].mean(axis=1)\n",
    "all_df['min'] = all_df[features].min(axis=1)\n",
    "all_df['max'] = all_df[features].max(axis=1)\n",
    "all_df['std'] = all_df[features].std(axis=1)\n",
    "all_df['median'] = all_df[features].median(axis=1)\n",
    "all_df['skew'] = all_df[features].skew(axis=1)\n",
    "all_df['kurt'] = all_df[features].kurt(axis=1)\n",
    "\n",
    "statistical_features = ['mean', 'min', 'max', 'std', 'median', 'skew', 'kurt']\n",
    "# Due to normalization, mean and sum become the same value, so we only include one of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Normalizing features to have zero mean, unit variance can often speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b6c65e56434b37a6d88015bdcc4dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=207), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in tqdm(features + statistical_features):\n",
    "    if feature in features:\n",
    "        all_df[feature] = StandardScaler().fit_transform(all_df[feature].values.reshape(-1, 1))\n",
    "        all_df[f'{feature}_count'] = MinMaxScaler().fit_transform(all_df[f'{feature}_count'].values.reshape(-1, 1))\n",
    "    if feature in statistical_features:\n",
    "        all_df[feature] = MinMaxScaler().fit_transform(all_df[feature].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update feature list\n",
    "We update the feature list with our new features, so that they are included in training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(features)):\n",
    "    features.append(f'{features[f]}_count')\n",
    "features.extend(statistical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting datasets back up\n",
    "We are now done with our feature engineering, so we can split the data back into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = all_df.iloc[:train_df.shape[0], :]\n",
    "test_df = all_df.iloc[train_df.shape[0]:, :]\n",
    "\n",
    "del all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "We use a neural network model to make predictions. 5-fold stratified cross validation is used, and we rely on early stopping, setting a large default number of epochs of 100. Binary cross-entropy is used for the loss function, which is a good fit for a binary classification task such as this one. The output of the network is a single output from a sigmoid function, which can be interpreted as a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "EARLY_STOPPING_PATIENCE = 8\n",
    "\n",
    "OPTIMIZER = tf.keras.optimizers.Nadam()\n",
    "LOSS='binary_crossentropy'\n",
    "METRICS=[tf.keras.metrics.AUC()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cnn_model():\n",
    "#     model = tf.keras.models.Sequential([\n",
    "#         tf.keras.layers.Reshape((len(features) * 1, 1), input_shape=(len(features) * 1,)),\n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "#         #tf.keras.layers.Conv1D(64, 1, activation='relu'),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         #tf.keras.layers.Dense(128, activation='relu'),\n",
    "#         #tf.keras.layers.Conv1D(128, 1, activation='relu'),\n",
    "#         #tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Dense(256, activation='relu'),\n",
    "#         #tf.keras.layers.Conv1D(256, 1, activation='relu'),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         #tf.keras.layers.Dense(512, activation='relu'),\n",
    "#         #tf.keras.layers.Conv1D(512, 1, activation='relu'),\n",
    "#         #tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Dense(1024, activation='relu'),\n",
    "#         #tf.keras.layers.Conv1D(1024, 1, activation='relu'),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         #tf.keras.layers.Dense(2048, activation='relu'),\n",
    "#         #tf.keras.layers.Conv1D(2048, 1, activation='relu'),\n",
    "#         #tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Flatten(),\n",
    "#         tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "#     ])\n",
    "    \n",
    "#     return model\n",
    "def get_cnn_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Reshape((len(features) * 1, 1), input_shape=(len(features) * 1,)),\n",
    "        tf.keras.layers.Conv1D(32, 1, activation='elu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv1D(64, 1, activation='elu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.005)),\n",
    "        #tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        #tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        #tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# Due to memory issues with saving all models, we instead make the predictions as each fold is trained.\n",
    "#models = []\n",
    "histories = []\n",
    "oof_preds_sum = np.zeros((train_df.shape[0],))\n",
    "train_preds_sum = np.zeros((train_df.shape[0],))\n",
    "test_preds_sum = np.zeros((test_df.shape[0],))\n",
    "\n",
    "for fold_num, (train_index, val_index) in tqdm(enumerate(kfold.split(train_df[features].values, train_df['target'].values)), total=N_SPLITS):\n",
    "    print(f'Fold {fold_num+1}/{N_SPLITS}:')\n",
    "    \n",
    "    X_train = train_df.loc[train_index, features].values\n",
    "    y_train = train_df.loc[train_index, 'target'].values.reshape(-1, 1)\n",
    "    X_val = train_df.loc[val_index, features].values\n",
    "    y_val = train_df.loc[val_index, 'target'].values.reshape(-1, 1)\n",
    "    \n",
    "    model = get_cnn_model()\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
    "    \n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping_callback])\n",
    "    histories.append(history)\n",
    "    \n",
    "    \n",
    "    print(f'Creating predictions for fold {fold_num + 1}/{N_SPLITS}')\n",
    "    val_preds = model.predict(X_val)\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(test_df[features].values)\n",
    "    \n",
    "    oof_preds_sum[val_index] += val_preds[:, 0]\n",
    "    train_preds_sum[train_index] += train_preds[:, 0]\n",
    "    test_preds_sum += test_preds[:, 0]\n",
    "    \n",
    "    val_auc = roc_auc_score(y_val, val_preds)\n",
    "    print(f'Fold validation AUC: {val_auc}')\n",
    "    print()\n",
    "    #models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5 * (len(histories) // 5 + 1)))\n",
    "for i, h in enumerate(histories):\n",
    "    plt.subplot(len(histories) // 5 + 1, 5, i+1)\n",
    "    plt.plot(h.history['loss'], label='Train loss')\n",
    "    plt.plot(h.history['val_loss'], label='Val loss')\n",
    "    plt.plot(h.history['auc'], label='Train AUC')\n",
    "    plt.plot(h.history['val_auc'], label='Val AUC')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_preds = np.zeros(train_df.shape[0])\n",
    "# test_preds = np.zeros(test_df.shape[0])\n",
    "\n",
    "# for model in models:\n",
    "#     pred_train = model.predict(train_df[features].values)\n",
    "#     pred_test = model.predict(test_df[features].values)\n",
    "    \n",
    "#     train_preds += pred_train\n",
    "#     test_preds += pred_test\n",
    "\n",
    "# train_preds /= len(models)\n",
    "# test_preds /= len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_preds = train_preds[:, 0]\n",
    "#test_preds = test_preds[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_sum /= N_SPLITS\n",
    "oof_preds_sum /= N_SPLITS\n",
    "test_preds_sum /= N_SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = roc_auc_score(train_df['target'], train_preds_sum)\n",
    "oof_auc = roc_auc_score(train_df['target'], oof_preds_sum)\n",
    "print(f'Train AUC: {train_auc}')\n",
    "print(f'Out of fold AUC: {oof_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempt to load the small test set with targets (reverse engineered from train set), which allows us to evaluate test set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_df = pd.read_csv('test_small_with_targets.csv')\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_df['target'][0] != -1:\n",
    "    test_auc = roc_auc_score(test_df['target'], test_preds_sum)\n",
    "    print(f'Test AUC: {test_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'ID_code': test_df['ID_code'], 'target': test_preds_sum})\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
