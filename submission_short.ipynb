{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style('dark')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "We set a few flags for whether we are running locally and on Kaggle, and whether we want the reduced or full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "IS_LOCAL = True  # Sets whether we are running locally or on kaggle\n",
    "USE_REDUCED = True  # Sets whether we should use the smaller dataset\n",
    "\n",
    "data_index = 2*int(IS_LOCAL) + int(USE_REDUCED)\n",
    "train_path = ('../input/santander-customer-transaction-prediction/train.csv',\n",
    "             '../input/santandersmall/train_small.csv',\n",
    "             'train.csv',\n",
    "             'train_small.csv')[data_index]\n",
    "test_path = ('../input/santander-customer-transaction-prediction/test.csv',\n",
    "             '../input/santandersmall/test_small_with_targets.csv',\n",
    "             'test.csv',\n",
    "             'test_small.csv')[data_index]\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in train_df.columns if col not in ['target', 'ID_code']]\n",
    "if not 'target' in test_df:\n",
    "    test_df['target'] = -1\n",
    "\n",
    "all_df = pd.concat([train_df, test_df], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing fake test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987200fc4d0847a7aff3fad526549c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real: 70065\n",
      "Synthetic: 0\n"
     ]
    }
   ],
   "source": [
    "unique_count = np.zeros((test_df.shape[0], len(features)))\n",
    "\n",
    "for f, feature in tqdm(enumerate(features), total=len(features)):\n",
    "    _, i, c = np.unique(test_df[feature], return_counts=True, return_index=True)\n",
    "    unique_count[i[c == 1], f] += 1\n",
    "\n",
    "real_sample_indices = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n",
    "synthetic_sample_indices = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n",
    "print('Real:', len(real_sample_indices))\n",
    "print('Synthetic:', len(synthetic_sample_indices))\n",
    "\n",
    "del unique_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1bb199536a4b3cb10b167b1897b195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_real_df = pd.concat([train_df, test_df.iloc[real_sample_indices, :]], sort=False)\n",
    "\n",
    "for feature in tqdm(features):\n",
    "    real_series = all_real_df[feature]\n",
    "    \n",
    "    # We only use the real samples to produce the count\n",
    "    counts = real_series.groupby(real_series).count()\n",
    "    \n",
    "    full_series = all_df[feature]\n",
    "    all_df[f'{feature}_count'] = full_series.map(counts)\n",
    "\n",
    "del all_real_df\n",
    "del real_series\n",
    "del full_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['sum'] = all_df[features].sum(axis=1)\n",
    "all_df['mean'] = all_df[features].mean(axis=1)\n",
    "all_df['min'] = all_df[features].min(axis=1)\n",
    "all_df['max'] = all_df[features].max(axis=1)\n",
    "all_df['std'] = all_df[features].std(axis=1)\n",
    "all_df['median'] = all_df[features].median(axis=1)\n",
    "all_df['skew'] = all_df[features].skew(axis=1)\n",
    "all_df['kurt'] = all_df[features].kurt(axis=1)\n",
    "\n",
    "statistical_features = ['mean', 'min', 'max', 'std', 'median', 'skew', 'kurt']\n",
    "# Due to normalization, mean and sum become the same value, so we only include one of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cef930c1b3f43a08702392c36f07408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=207), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in tqdm(features + statistical_features):\n",
    "    if feature in features:\n",
    "        all_df[feature] = StandardScaler().fit_transform(all_df[feature].values.reshape(-1, 1))\n",
    "        all_df[f'{feature}_count'] = MinMaxScaler().fit_transform(all_df[f'{feature}_count'].values.reshape(-1, 1))\n",
    "    if feature in statistical_features:\n",
    "        all_df[feature] = StandardScaler().fit_transform(all_df[feature].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(features)):\n",
    "    features.append(f'{features[f]}_count')\n",
    "features.extend(statistical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting datasets back up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = all_df.iloc[:train_df.shape[0], :]\n",
    "test_df = all_df.iloc[train_df.shape[0]:, :]\n",
    "\n",
    "del all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regularized_cnn_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Reshape((len(features) * 1, 1), input_shape=(len(features) * 1,)),\n",
    "        tf.keras.layers.Conv1D(32, 1, activation='elu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv1D(64, 1, activation='elu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.005)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "EARLY_STOPPING_PATIENCE = 8\n",
    "\n",
    "OPTIMIZER = tf.keras.optimizers.Nadam()\n",
    "LOSS='binary_crossentropy'\n",
    "METRICS=[tf.keras.metrics.AUC()]\n",
    "\n",
    "model_fn = get_regularized_cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d1cb77cd7143fbb3a8c38c98fc7777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5:\n",
      "Train on 56045 samples, validate on 14012 samples\n",
      "Epoch 1/100\n",
      "56045/56045 [==============================] - 24s 426us/sample - loss: 2.5632 - auc: 0.7391 - val_loss: 1.0295 - val_auc: 0.4991\n",
      "Epoch 2/100\n",
      "56045/56045 [==============================] - 17s 304us/sample - loss: 0.6142 - auc: 0.8385 - val_loss: 0.6509 - val_auc: 0.5351\n",
      "Epoch 3/100\n",
      "56045/56045 [==============================] - 17s 303us/sample - loss: 0.3795 - auc: 0.8420 - val_loss: 0.4349 - val_auc: 0.8215\n",
      "Epoch 4/100\n",
      "56045/56045 [==============================] - 17s 305us/sample - loss: 0.3518 - auc: 0.8458 - val_loss: 0.3692 - val_auc: 0.8389\n",
      "Epoch 5/100\n",
      "56045/56045 [==============================] - 17s 307us/sample - loss: 0.3629 - auc: 0.8453 - val_loss: 0.3689 - val_auc: 0.8399\n",
      "Epoch 6/100\n",
      "56045/56045 [==============================] - 17s 305us/sample - loss: 0.3762 - auc: 0.8476 - val_loss: 0.3840 - val_auc: 0.8402\n",
      "Epoch 7/100\n",
      "56045/56045 [==============================] - 17s 304us/sample - loss: 0.4003 - auc: 0.8492 - val_loss: 0.3839 - val_auc: 0.8539\n",
      "Epoch 8/100\n",
      "56045/56045 [==============================] - 18s 314us/sample - loss: 0.4073 - auc: 0.8534 - val_loss: 0.4513 - val_auc: 0.8402\n",
      "Epoch 9/100\n",
      "56045/56045 [==============================] - 17s 310us/sample - loss: 0.4138 - auc: 0.8564 - val_loss: 0.4249 - val_auc: 0.8449\n",
      "Epoch 10/100\n",
      "56045/56045 [==============================] - 17s 311us/sample - loss: 0.4214 - auc: 0.8623 - val_loss: 0.4445 - val_auc: 0.8347\n",
      "Epoch 11/100\n",
      "56045/56045 [==============================] - 18s 313us/sample - loss: 0.4346 - auc: 0.8633 - val_loss: 0.5029 - val_auc: 0.8552\n",
      "Epoch 12/100\n",
      "56045/56045 [==============================] - 18s 313us/sample - loss: 0.4216 - auc: 0.8662 - val_loss: 0.4464 - val_auc: 0.8581\n",
      "Epoch 13/100\n",
      "45056/56045 [=======================>......] - ETA: 3s - loss: 0.4213 - auc: 0.8680"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# Due to memory issues with saving all models, we instead make the predictions as each fold is trained.\n",
    "#models = []\n",
    "histories = []\n",
    "oof_preds_sum = np.zeros((train_df.shape[0],))\n",
    "train_preds_sum = np.zeros((train_df.shape[0],))\n",
    "test_preds_sum = np.zeros((test_df.shape[0],))\n",
    "\n",
    "for fold_num, (train_index, val_index) in tqdm(enumerate(kfold.split(train_df[features].values, train_df['target'].values)), total=N_SPLITS):\n",
    "    print(f'Fold {fold_num+1}/{N_SPLITS}:')\n",
    "    \n",
    "    X_train = train_df.loc[train_index, features].values\n",
    "    y_train = train_df.loc[train_index, 'target'].values.reshape(-1, 1)\n",
    "    X_val = train_df.loc[val_index, features].values\n",
    "    y_val = train_df.loc[val_index, 'target'].values.reshape(-1, 1)\n",
    "    \n",
    "    model = model_fn()\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
    "    \n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping_callback])\n",
    "    histories.append(history)\n",
    "    \n",
    "    \n",
    "    print(f'Creating predictions for fold {fold_num + 1}/{N_SPLITS}')\n",
    "    val_preds = model.predict(X_val)\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(test_df[features].values)\n",
    "    \n",
    "    oof_preds_sum[val_index] += val_preds[:, 0]\n",
    "    train_preds_sum[train_index] += train_preds[:, 0]\n",
    "    test_preds_sum += test_preds[:, 0]\n",
    "    \n",
    "    val_auc = roc_auc_score(y_val, val_preds)\n",
    "    print(f'Fold validation AUC: {val_auc}')\n",
    "    print()\n",
    "    #models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'ID_code': test_df['ID_code'], 'target': test_preds_sum})\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
